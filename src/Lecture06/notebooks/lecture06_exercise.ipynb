{"cells":[{"cell_type":"markdown","metadata":{"id":"0NWVFkhv-EW2"},"source":["# 第6回講義 演習\n","\n","本演習ではPyTorchを使用し，基本的な回帰結合型ニューラルネット(RNN)と，その一種である長短期記憶(LSTM)を実装し，文章の分類タスクを解いていきます．\n","\n","IMDb (Internet Movie Database) と呼ばれるデータセットには，映画のレビュー文とその評価がpositiveかnegativeかが記録されています．\n","\n","<div style=\"text-align: center;\">【データセットのイメージ】</div>\n","\n","| レビュー | 評価 |\n","|:--------:|:-------------:|\n","|Where's Michael Caine when you need him? I've ...|negative|\n","|To experience Head you really need to understa...|positive|\n","\n","そこで各レビュー文を入力として，その評価positive/negativeの二値分類をRNNで行ってみましょう．"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24714,"status":"ok","timestamp":1715757778579,"user":{"displayName":"飯山燈","userId":"03970948079277386100"},"user_tz":-540},"id":"AP7Iwy3PFZ3m","outputId":"ef5e130e-153b-4288-e8c9-744af7b2e2a4"},"outputs":[{"ename":"OSError","evalue":"dlopen(/Users/keisuketoyoda/utokyo/graduate/deep_learning/venv/lib/python3.9/site-packages/torchtext/lib/libtorchtext.so, 0x0006): Symbol not found: __ZN3c104impl3cow23materialize_cow_storageERNS_11StorageImplE\n  Referenced from: <DF3ABA20-1A43-316C-A7EC-B234DC9E3163> /Users/keisuketoyoda/utokyo/graduate/deep_learning/venv/lib/python3.9/site-packages/torchtext/lib/libtorchtext.so\n  Expected in:     <E4A087BF-2A73-36B5-9B71-18B1A776D3D4> /Users/keisuketoyoda/utokyo/graduate/deep_learning/venv/lib/python3.9/site-packages/torch/lib/libc10.dylib","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequence\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets\n","File \u001b[0;32m~/utokyo/graduate/deep_learning/venv/lib/python3.9/site-packages/torchtext/__init__.py:18\u001b[0m\n\u001b[1;32m     15\u001b[0m     _WARN \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# the following import has to happen first in order to load the torchtext C++ library\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _extension  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     20\u001b[0m _TEXT_BUCKET \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://download.pytorch.org/models/text/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     22\u001b[0m _CACHE_DIR \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(_get_torch_home(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n","File \u001b[0;32m~/utokyo/graduate/deep_learning/venv/lib/python3.9/site-packages/torchtext/_extension.py:64\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _torchtext  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m \u001b[43m_init_extension\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/utokyo/graduate/deep_learning/venv/lib/python3.9/site-packages/torchtext/_extension.py:58\u001b[0m, in \u001b[0;36m_init_extension\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _mod_utils\u001b[38;5;241m.\u001b[39mis_module_available(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchtext._torchtext\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchtext C++ Extension is not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlibtorchtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _torchtext\n","File \u001b[0;32m~/utokyo/graduate/deep_learning/venv/lib/python3.9/site-packages/torchtext/_extension.py:50\u001b[0m, in \u001b[0;36m_load_lib\u001b[0;34m(lib)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n","File \u001b[0;32m~/utokyo/graduate/deep_learning/venv/lib/python3.9/site-packages/torch/_ops.py:933\u001b[0m, in \u001b[0;36mload_library\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    928\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_OpNamespace\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mop_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    929\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    931\u001b[0m \u001b[38;5;66;03m# let the script frontend know that op is identical to the builtin op\u001b[39;00m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;66;03m# with qualified_op_name\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39m_builtins\u001b[38;5;241m.\u001b[39m_register_builtin(op, qualified_op_name)\n\u001b[1;32m    934\u001b[0m op\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m namespace_name\n\u001b[1;32m    935\u001b[0m opoverloadpacket \u001b[38;5;241m=\u001b[39m OpOverloadPacket(\n\u001b[1;32m    936\u001b[0m     qualified_op_name, op_name, op, overload_names\n\u001b[1;32m    937\u001b[0m )\n","File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/ctypes/__init__.py:366\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 366\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    368\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n","\u001b[0;31mOSError\u001b[0m: dlopen(/Users/keisuketoyoda/utokyo/graduate/deep_learning/venv/lib/python3.9/site-packages/torchtext/lib/libtorchtext.so, 0x0006): Symbol not found: __ZN3c104impl3cow23materialize_cow_storageERNS_11StorageImplE\n  Referenced from: <DF3ABA20-1A43-316C-A7EC-B234DC9E3163> /Users/keisuketoyoda/utokyo/graduate/deep_learning/venv/lib/python3.9/site-packages/torchtext/lib/libtorchtext.so\n  Expected in:     <E4A087BF-2A73-36B5-9B71-18B1A776D3D4> /Users/keisuketoyoda/utokyo/graduate/deep_learning/venv/lib/python3.9/site-packages/torch/lib/libc10.dylib"]}],"source":["import random\n","import numpy as np\n","import string\n","import re\n","from collections import Counter\n","from typing import List\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.autograd as autograd\n","from torch.utils.data import DataLoader\n","import torch.nn.functional as F\n","from torch.nn.utils.rnn import pad_sequence\n","import torchtext\n","from torchtext import data\n","from torchtext import datasets\n","from torchtext.vocab import vocab\n","from torchtext.data.utils import get_tokenizer\n","from sklearn.metrics import f1_score\n","\n","seed = 1234\n","torch.manual_seed(seed)\n","np.random.seed(seed)\n","random.seed(seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j2xVBypuxc-M"},"outputs":[],"source":["# torch.log(0)によるnanを防ぐための関数\n","def torch_log(x):\n","    return torch.log(torch.clamp(x, min=1e-10))"]},{"cell_type":"markdown","metadata":{"id":"flBww3ZJgmNS"},"source":["IMDbは`torchtext==0.15.1`からラベルの扱いが変わっています．`torchtext`のバージョンが`0.15.1`以上であることを確認してください．"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1164,"status":"ok","timestamp":1682756658484,"user":{"displayName":"Sensho Nobe","userId":"10041880977514616020"},"user_tz":-540},"id":"WXmoH5yuwNjs","outputId":"d82a52db-4896-4f35-bc02-cb05a6f16ffa"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch                         2.0.0+cu118\n","torchaudio                    2.0.1+cu118\n","torchdata                     0.6.0\n","torchsummary                  1.5.1\n","torchtext                     0.15.1\n","torchvision                   0.15.1+cu118\n"]}],"source":["!pip list | grep torch"]},{"cell_type":"markdown","metadata":{"id":"Z-aoVICDcH2W"},"source":["## 目次\n","\n","1. [データセットの読み込み](#scrollTo=zEltPNkLuLz6)\n","1. [訓練実行(trainer)関数の定義](#scrollTo=G-laa4YqQRji)\n","\n","【課題1】Recurrent Neural Network (RNN) によるIMDbのsentiment analysis\n","\n","1. [Embedding層](#scrollTo=Urh6GUOQKBzE)  \n","1. [RNN](#scrollTo=ZNPaK9ExKBzI)  \n","1. [分類器](#scrollTo=2O0bZWqVOVk0)\n","1. [学習](#scrollTo=emiO4f5rCklA)\n","1. [torch.nn.RNN, torch.nn.Embeddingを用いたネットワークの記述](#scrollTo=yCktWJ9N8QDN)\n","\n","【課題2】Long short-term memory (LSTM) によるIMDbのsentiment analysis\n","1. [LSTM](#scrollTo=tsXtYkNEm1Bh)\n","1. [分類器](#scrollTo=IfHaLvJJWHeI)\n","1. [学習](#scrollTo=qR2iKUy7yA3R)\n","1. [torch.nn.LSTMを用いたネットワークの記述](#scrollTo=thf8W0lywagD)\n","\n","【課題3】Bidirectional LSTM\n","1. [BidirectionalLSTM](#scrollTo=L9ZNn5gTP2yH)\n","1. [学習](#scrollTo=1xbAI25fQA2E)\n","\n","【補足】[Gradient Clippingによる長系列への対処](#scrollTo=ExuiSiTo2k3m)"]},{"cell_type":"markdown","metadata":{"id":"zEltPNkLuLz6"},"source":["## 1. データセットの読み込み\n","\n","自然言語処理において，データとなる文をそのままネットワークに入力することは出来ないので，適切な前処理をする必要があります．\n","\n","前処理の手順（英文の場合）は大まかに，\n","- 単語ごとに区切る\n","- 各単語にIDを割り振る\n","\n","という手順で行われ，この手順を経ることで，元々の文は整数列に変換され，ネットワークに入力することが可能となります．\n","\n","また，本演習では各単語にIDを割り振る処理に`torchtext`と呼ばれるライブラリを用いています．詳しく知りたい方は，[公式ドキュメント](https://pytorch.org/text/)を参照してください．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9oN38u4fwNju"},"outputs":[],"source":["train_iter = datasets.IMDB(split='train')\n","\n","train_iter, valid_iter = train_iter.random_split(\n","    weights={\"train\": 0.8, \"valid\": 0.2},\n","    seed=seed,\n","    total_length=len(list(train_iter)),\n",")"]},{"cell_type":"markdown","metadata":{"id":"oeotUWUJx9_-"},"source":["訓練データの文章内の単語を[collections.Counter](https://docs.python.org/ja/3/library/collections.html?highlight=collections%20counter#collections.Counter)で数え上げ，[torchtext.vocab.vocab](https://pytorch.org/text/stable/vocab.html#id1)で語彙リスト（`Vocab`オブジェクト）を作っていきます．\n","\n","引数`specials`で特別なトークンを指定しています．\n","- `<unk>`: Unknown. 出てくる頻度が少なすぎる単語などを未分類としておきます．\n","- `<PAD>`: 短い文章はこのトークンで埋めることで，長い文章に長さを合わせます．\n","- `<BOS>`: Begin of sentence.\n","- `<EOS>`: End of sentence."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5405,"status":"ok","timestamp":1682756663886,"user":{"displayName":"Sensho Nobe","userId":"10041880977514616020"},"user_tz":-540},"id":"S_Wl5lKUwNju","outputId":"4a3d568a-2eae-404a-8f34-8790947f6151"},"outputs":[{"name":"stdout","output_type":"stream","text":["単語種数: 9937\n","<unk>, <PAD>, <BOS>, <EOS>, i, am, curious, yellow, is, a, and, pretentious, steaming, pile, ., it, doesn, ', t, matter, what, one, s, political, views, are, because, this, film, can, hardly, be, taken, seriously, on, any, level, as, for, the, claim, that, frontal, male, nudity, an, automatic, ,, isn, true, ve, seen, films, with, granted, they, only, offer, some, fleeting, but, where, ?, nowhere, don, exist, same, goes, those, crappy, cable, shows, swinging, in, not, sight, indie, movies, like, brown, bunny, which, we, re, treated, to, site, of, vincent, johnson, trace, pink, visible, chloe, before, crying, (, or, ), matters\n"]}],"source":["# 単語をスペースで区切り，!\"#$%&といった記号を除去する，すべて小文字化する，などの処理\n","# https://pytorch.org/text/stable/data_utils.html\n","tokenizer = get_tokenizer(\"basic_english\")\n","\n","counter = Counter()\n","\n","for label, line in train_iter:\n","    counter.update(tokenizer(line))\n","\n","vocabulary = vocab(\n","    counter,\n","    min_freq=25,\n","    specials=('<unk>', '<PAD>', '<BOS>', '<EOS>')\n",")\n","# <unk>をデフォルトに設定することにより，min_freq回以上出てこない単語は<unk>になる\n","vocabulary.set_default_index(vocabulary['<unk>'])\n","\n","word_num = len(vocabulary)\n","\n","print(f\"単語種数: {word_num}\")\n","print(*vocabulary.get_itos()[:100], sep=', ')"]},{"cell_type":"markdown","metadata":{"id":"6eXWV-W9fsJ2"},"source":["`text_transform()`を`collate_batch()`から呼び，単語のリストを辞書内インデックスのリストに変換します．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"waztS-jcwNjv"},"outputs":[],"source":["def text_transform(_text, max_length=256):\n","    # <BOS>と<EOS>の分 -2\n","    text = [vocabulary[token] for token in tokenizer(_text)][:max_length - 2]\n","    text = [vocabulary['<BOS>']] + text + [vocabulary['<EOS>']]\n","\n","    return text, len(text)\n","\n","def collate_batch(batch):\n","   label_list, text_list, len_seq_list = [], [], []\n","\n","   for _label, _text in batch:\n","      # torchtext==0.14.0まではnegativeは'neg', positiveは'pos'\n","      # torchtext==0.15.1からはnegativeは1，positiveは2なので，-1して{0, 1}にする\n","      label_list.append(_label - 1)\n","\n","      processed_text, len_seq = text_transform(_text)\n","      text_list.append(torch.tensor(processed_text))\n","      len_seq_list.append(len_seq)\n","\n","   return torch.tensor(label_list), pad_sequence(text_list, padding_value=1).T, torch.tensor(len_seq_list)"]},{"cell_type":"markdown","metadata":{"id":"A4wrvRPx_k4H"},"source":["上で定義した`collate_batch()`を`DataLoader`に渡すことで，バッチに対してその処理を適用することができます．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-zNihuaPSuNl"},"outputs":[],"source":["batch_size = 128\n","\n","train_dataloader = DataLoader(\n","   list(train_iter),\n","   batch_size=batch_size,\n","   shuffle=True,\n","   collate_fn=collate_batch\n",")\n","valid_dataloader = DataLoader(\n","   list(valid_iter),\n","   batch_size=batch_size,\n","   shuffle=False,\n","   collate_fn=collate_batch\n",")"]},{"cell_type":"markdown","metadata":{"id":"G-laa4YqQRji"},"source":["## 2. 訓練実行(trainer)関数の定義\n","\n","以降定義するすべてのモデルについて訓練ループは同じなので，ここで関数として実装してしまいます．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qmcVzxAvQbdJ"},"outputs":[],"source":["# NOTE: dataloaderはグローバルスコープ\n","def train(\n","    net,\n","    optimizer,\n","    n_epochs,\n","):\n","    for epoch in range(n_epochs):\n","        losses_train = []\n","        losses_valid = []\n","\n","        net.train()\n","        n_train = 0\n","        acc_train = 0\n","        for label, line, len_seq in train_dataloader:\n","            net.zero_grad()  # 勾配の初期化\n","\n","            t = label.to(device) # テンソルをGPUに移動\n","            x = line.to(device) # ( batch, time )\n","            len_seq.to(device)\n","\n","            h = net(x, torch.max(len_seq), len_seq)\n","            y = torch.sigmoid(h).squeeze()\n","\n","            loss = # WITE ME\n","\n","            loss.backward()  # 誤差の逆伝播\n","\n","            optimizer.step()  # パラメータの更新\n","\n","            losses_train.append(loss.tolist())\n","\n","            n_train += t.size()[0]\n","\n","        # Valid\n","        t_valid = []\n","        y_pred = []\n","        net.eval()\n","        for label, line, len_seq in valid_dataloader:\n","\n","            t = label.to(device) # テンソルをGPUに移動\n","            x = line.to(device)\n","            len_seq.to(device)\n","\n","            h = net(x, torch.max(len_seq), len_seq)\n","            y = torch.sigmoid(h).squeeze()\n","\n","            loss = # WRITE ME\n","\n","            pred = y.round().squeeze()  # 0.5以上の値を持つ要素を正ラベルと予測する\n","\n","            t_valid.extend(t.tolist())\n","            y_pred.extend(pred.tolist())\n","\n","            losses_valid.append(loss.tolist())\n","\n","        print('EPOCH: {}, Train Loss: {:.3f}, Valid Loss: {:.3f}, Validation F1: {:.3f}'.format(\n","            epoch,\n","            np.mean(losses_train),\n","            np.mean(losses_valid),\n","            f1_score(t_valid, y_pred, average='macro')\n","        ))"]},{"cell_type":"markdown","metadata":{"id":"p5oyIOujKBy0"},"source":["## 課題1. Recurrent Neural Network (RNN) によるIMDbのsentiment analysis"]},{"cell_type":"markdown","metadata":{"id":"Urh6GUOQKBzE"},"source":["### 1. Embedding層\n","\n","`Embedding`層では，単語を離散的なidから連続的な数百次元のベクトルに変換(埋め込み; embedding)します．\n","\n","下の`Embedding`クラスにおいて，入力$\\boldsymbol{x}$は各行に文の単語のid列が入った行列で，重み$\\boldsymbol{V}$は各行がそれぞれの単語idのベクトルに対応した行列です．\n","\n","つまりそれぞれの行列のサイズは\n","\n","- $\\boldsymbol{x}$: (ミニバッチサイズ) x (ミニバッチ内の文の最大系列長)\n","- $\\boldsymbol{V}$: (辞書の単語数) x (単語のベクトルの次元数)\n","\n","です．\n","\n","この$\\boldsymbol{V}$から，入力$\\boldsymbol{x}$のそれぞれの単語idに対して対応する単語ベクトルを取り出すことで，各単語をベクトルに変換します．\n","\n","この処理によって出力されるテンソルの次元数は，(ミニバッチサイズ) x (ミニバッチ内の文の最大系列長) x (単語のベクトルの次元数)となります．\n","\n","![embedding](../figures/embedding.png)\n","\n","$$m:\\text{emb_dim}, \\ n : \\text{vocab_size}$$\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xsrziOUkFX07"},"outputs":[],"source":["class Embedding(nn.Module):\n","    def __init__(self, emb_dim, vocab_size):\n","        super().__init__()\n","        self.embedding_matrix = nn.Parameter(torch.rand((vocab_size, emb_dim),\n","                                                        dtype=torch.float))\n","\n","    def forward(self, x):\n","        return F.embedding(x, self.embedding_matrix)"]},{"cell_type":"markdown","metadata":{"id":"ZNPaK9ExKBzI"},"source":["### 2. RNN\n","\n","RNNクラスでは，Embedding層で各単語がベクトルに変換されたものを入力として処理を行います．ここで入力$\\boldsymbol{x}$は\n","\n","- $\\boldsymbol{x}$: (ミニバッチサイズ) x (ミニバッチ内の文の最大系列長) x (単語のベクトルの次元数)\n","\n","となっています．\n","\n","ここでは `nn.Module` を用いてRNNクラスを定義します．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qq01kXOFw3hD"},"outputs":[],"source":["class RNN(nn.Module):\n","    def __init__(self, in_dim, hid_dim):\n","        super().__init__()\n","        self.hid_dim = hid_dim\n","        glorot = 6 / (in_dim + hid_dim*2)\n","        self.W = nn.Parameter(torch.tensor(np.random.uniform(\n","                        low=-np.sqrt(glorot),\n","                        high=np.sqrt(glorot),\n","                        size=(in_dim + hid_dim, hid_dim)\n","                    ).astype('float32')))\n","        self.b = nn.Parameter(torch.tensor(np.zeros([hid_dim]).astype('float32')))\n","\n","    def function(self, h, x):\n","        return torch.tanh(torch.matmul(torch.cat([h, x], dim=1), self.W) + self.b)\n","\n","    def forward(self, x, len_seq_max=0, init_state=None):\n","        x = x.transpose(0, 1)  # 系列のバッチ処理のため、次元の順番を「系列、バッチ」の順に入れ替える\n","        state = init_state\n","\n","        if init_state is None:  # 初期値を設定しない場合は0で初期化する\n","            state = torch.zeros((x[0].size()[0], self.hid_dim)).to(x.device)\n","\n","        size = list(state.unsqueeze(0).size())\n","        size[0] = 0\n","        output = torch.empty(size, dtype=torch.float).to(x.device)  # 一旦空テンソルを定義して順次出力を追加する\n","\n","        if len_seq_max == 0:\n","            len_seq_max = x.size(0)\n","        for i in range(len_seq_max):\n","            state = self.function(state, x[i])\n","            output = torch.cat([output, state.unsqueeze(0)])  # 出力系列の追加\n","        return output"]},{"cell_type":"markdown","metadata":{"id":"2O0bZWqVOVk0"},"source":["### 3. 分類器"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YpDGYK_rEcVS"},"outputs":[],"source":["class SequenceTaggingNet(nn.Module):\n","    def __init__(self, word_num, emb_dim, hid_dim):\n","        super().__init__()\n","        self.emb = Embedding(emb_dim, word_num)\n","        self.rnn = RNN(emb_dim, hid_dim)\n","        self.linear = nn.Linear(hid_dim, 1)\n","\n","    def forward(self, x, len_seq_max=0, len_seq=None, init_state=None):\n","        h = self.emb(x)\n","        h = self.rnn(h, len_seq_max, init_state)\n","        if len_seq is not None:\n","            # 系列が終わった時点での出力を取る必要があるので len_seq を元に集約する\n","            h = h[len_seq - 1, list(range(len(x))), :]\n","        else:\n","            h = h[-1]\n","        y = self.linear(h)\n","        return y"]},{"cell_type":"markdown","metadata":{"id":"emiO4f5rCklA"},"source":["### 4. 学習"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":297419,"status":"ok","timestamp":1682756963772,"user":{"displayName":"Sensho Nobe","userId":"10041880977514616020"},"user_tz":-540},"id":"NF1oS8scU160","outputId":"834e3f27-ccf4-4009-d26f-ce5b405dc456"},"outputs":[{"name":"stdout","output_type":"stream","text":["EPOCH: 0, Train Loss: 0.691, Valid Loss: 0.678, Validation F1: 0.547\n","EPOCH: 1, Train Loss: 0.670, Valid Loss: 0.689, Validation F1: 0.506\n","EPOCH: 2, Train Loss: 0.661, Valid Loss: 0.674, Validation F1: 0.567\n","EPOCH: 3, Train Loss: 0.601, Valid Loss: 0.671, Validation F1: 0.598\n","EPOCH: 4, Train Loss: 0.531, Valid Loss: 0.694, Validation F1: 0.563\n","EPOCH: 5, Train Loss: 0.466, Valid Loss: 0.716, Validation F1: 0.631\n","EPOCH: 6, Train Loss: 0.439, Valid Loss: 0.699, Validation F1: 0.673\n","EPOCH: 7, Train Loss: 0.347, Valid Loss: 0.785, Validation F1: 0.638\n","EPOCH: 8, Train Loss: 0.294, Valid Loss: 0.853, Validation F1: 0.627\n","EPOCH: 9, Train Loss: 0.254, Valid Loss: 0.889, Validation F1: 0.647\n"]}],"source":["emb_dim = 100\n","hid_dim = 50\n","n_epochs = 10\n","device = 'cuda'\n","\n","net = SequenceTaggingNet(word_num, emb_dim, hid_dim)\n","net.to(device)\n","\n","optimizer = optim.Adam(net.parameters())\n","\n","train(net, optimizer, n_epochs)"]},{"cell_type":"markdown","metadata":{"id":"yCktWJ9N8QDN"},"source":["### 5. `torch.nn.RNN`, `torch.nn.Embedding` を用いたネットワークの記述\n","\n","`torch.nn.Conv2d`を用いてCNN層を容易に実装することができたように，`torch.nn.RNN`を用いることで容易にRNN層を実装することができます．\n","\n","`torch.nn.RNN`は，系列データ $x$ と初期状態 $h_0$ を引数として受け取り，出力系列 $y$ と最終状態 $h$ を出力します．（`y, h = self.RNN(x, h_0)`）\n","\n","また，少し厄介なことに`nn.RNN`の入力の系列データは，デフォルトで次元が [系列，ミニバッチ，各要素のベクトル] の順番となっています．しかしながら，データローダーが与えるデータは [ミニバッチ，系列，各要素] の順番となっており，この順番の方が直感的であると考えられます．これを解決するため，`torch.nn.RNN`は引数`batch_first`を受け取ることができ，これを`True`にすることで入力系列を [ミニバッチ，系列，各要素] の順番で受け取ることができます．\n","\n","`batch_first=True`とすると出力の順番も入れ替わるので，`transpose`を用いて [系列，ミニバッチ，各要素のベクトル] としてから`len_seq`を元に最終出力を取り出します．\n","\n","さらに，`torch.nn.RNN`は引数`num_layers`を持ち，RNNを何層重ねるかを指定します．多層に重ねることもできますが，ここでは課題2の実装と揃えるため一層とします．\n","\n","また，`torch.nn.Embedding`を用いることでEmbedding層も容易に実装することができます．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xou_yoP8Uxrs"},"outputs":[],"source":["class SequenceTaggingNet2(nn.Module):\n","    def __init__(self, word_num, emb_dim, hid_dim):\n","        super().__init__()\n","        self.emb = nn.Embedding(word_num, emb_dim)  # nn.Embeddingの使用\n","        self.rnn = nn.RNN(emb_dim, hid_dim, 1, batch_first=True)  # nn.RNNの使用\n","        self.linear = nn.Linear(hid_dim, 1)\n","\n","    def forward(self, x, len_seq_max=0, len_seq=None, init_state=None):\n","        h = # WRITE ME\n","\n","        if len_seq_max > 0:\n","            h, _ = self.rnn(h[:, 0:len_seq_max, :], init_state)\n","        else:\n","            h, _ = self.rnn(h, init_state)\n","        h = h.transpose(0, 1)\n","        if len_seq is None:\n","            # 系列が終わった時点での出力を取る必要があるので len_seq を元に集約する\n","            h = h[len_seq - 1, list(range(len(x))), :]\n","        else:\n","            h = h[-1]\n","\n","        y = # WRITE ME\n","\n","        return y"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":93917,"status":"ok","timestamp":1682757057684,"user":{"displayName":"Sensho Nobe","userId":"10041880977514616020"},"user_tz":-540},"id":"hlab8ft3U0Fq","outputId":"ed80b81f-32e4-474c-a78f-04065a41f3df"},"outputs":[{"name":"stdout","output_type":"stream","text":["EPOCH: 0, Train Loss: 0.696, Valid Loss: 0.695, Validation F1: 0.481\n","EPOCH: 1, Train Loss: 0.692, Valid Loss: 0.695, Validation F1: 0.451\n","EPOCH: 2, Train Loss: 0.688, Valid Loss: 0.698, Validation F1: 0.406\n","EPOCH: 3, Train Loss: 0.677, Valid Loss: 0.706, Validation F1: 0.441\n","EPOCH: 4, Train Loss: 0.659, Valid Loss: 0.719, Validation F1: 0.449\n","EPOCH: 5, Train Loss: 0.634, Valid Loss: 0.735, Validation F1: 0.467\n","EPOCH: 6, Train Loss: 0.603, Valid Loss: 0.778, Validation F1: 0.457\n","EPOCH: 7, Train Loss: 0.572, Valid Loss: 0.808, Validation F1: 0.468\n","EPOCH: 8, Train Loss: 0.542, Valid Loss: 0.865, Validation F1: 0.463\n","EPOCH: 9, Train Loss: 0.519, Valid Loss: 0.913, Validation F1: 0.452\n"]}],"source":["emb_dim = 100\n","hid_dim = 50\n","n_epochs = 10\n","device = 'cuda'\n","\n","net = SequenceTaggingNet2(word_num, emb_dim, hid_dim)\n","net.to(device)\n","\n","optimizer = optim.Adam(net.parameters())\n","\n","train(net, optimizer, n_epochs)"]},{"cell_type":"markdown","metadata":{"id":"RzcDxaEvmu_g"},"source":["## 課題2. Long short-term memory (LSTM)"]},{"cell_type":"markdown","metadata":{"id":"tsXtYkNEm1Bh"},"source":["### 1. LSTM\n","\n","実装する式は次のようになります．($\\odot$は要素ごとの積)\n","\n","- 入力ゲート: $\\hspace{20mm}\\boldsymbol{i}_t = \\mathrm{\\sigma} \\left(\\boldsymbol{W}_i \\left[\\begin{array}{c} \\boldsymbol{x}_t \\\\ \\boldsymbol{h}_{t-1} \\end{array}\\right] + \\boldsymbol{b}_i\\right)$\n","- 忘却ゲート: $\\hspace{20mm}\\boldsymbol{f}_t = \\mathrm{\\sigma} \\left(\\boldsymbol{W}_f \\left[\\begin{array}{c} \\boldsymbol{x}_t \\\\ \\boldsymbol{h}_{t-1} \\end{array}\\right] + \\boldsymbol{b}_f\\right)$  \n","- 出力ゲート: $\\hspace{20mm}\\boldsymbol{o}_t = \\mathrm{\\sigma} \\left(\\boldsymbol{W}_o \\left[\\begin{array}{c} \\boldsymbol{x}_t \\\\ \\boldsymbol{h}_{t-1} \\end{array}\\right] + \\boldsymbol{b}_o\\right)$  \n","- セル:　　　 $\\hspace{20mm}\\boldsymbol{c}_t = \\boldsymbol{f}_t \\odot \\boldsymbol{c}_{t-1} + \\boldsymbol{i}_t \\odot \\tanh \\left(\\boldsymbol{W}_c \\left[\\begin{array}{c} \\boldsymbol{x}_t \\\\ \\boldsymbol{h}_{t-1} \\end{array}\\right] + \\boldsymbol{b}_c\\right)$\n","- 隠れ状態: 　$\\hspace{20mm}\\boldsymbol{h}_t = \\boldsymbol{o}_t \\odot \\tanh \\left(\\boldsymbol{c}_t \\right)$\n","\n","単純なRNNでは各ステップの関数の戻り値は隠れ状態のみ ($\\boldsymbol{h}_t$) でしたが，LSTMではセル状態と隠れ状態の2つ ($\\boldsymbol{c}_t, \\boldsymbol{h}_t$) となるので注意してください．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Zp__4EDBoJ3"},"outputs":[],"source":["class LSTM(nn.Module):\n","    def __init__(self, in_dim, hid_dim):\n","        super().__init__()\n","        self.hid_dim = hid_dim\n","        glorot = 6/(in_dim + hid_dim*2)\n","\n","        self.W_i = nn.Parameter(torch.tensor(np.random.uniform(\n","                        low=-np.sqrt(glorot),\n","                        high=np.sqrt(glorot),\n","                        size=(in_dim + hid_dim, hid_dim)\n","                    ).astype('float32')))\n","        self.b_i = nn.Parameter(torch.tensor(np.zeros([hid_dim]).astype('float32')))\n","\n","        self.W_f = nn.Parameter(torch.tensor(np.random.uniform(\n","                        low=-np.sqrt(glorot),\n","                        high=np.sqrt(glorot),\n","                        size=(in_dim + hid_dim, hid_dim)\n","                    ).astype('float32')))\n","        self.b_f = nn.Parameter(torch.tensor(np.zeros([hid_dim]).astype('float32')))\n","\n","        self.W_o = nn.Parameter(torch.tensor(np.random.uniform(\n","                        low=-np.sqrt(glorot),\n","                        high=np.sqrt(glorot),\n","                        size=(in_dim + hid_dim, hid_dim)\n","                    ).astype('float32')))\n","        self.b_o = nn.Parameter(torch.tensor(np.zeros([hid_dim]).astype('float32')))\n","\n","        self.W_c = nn.Parameter(torch.tensor(np.random.uniform(\n","                        low=-np.sqrt(glorot),\n","                        high=np.sqrt(glorot),\n","                        size=(in_dim + hid_dim, hid_dim)\n","                    ).astype('float32')))\n","        self.b_c = nn.Parameter(torch.tensor(np.zeros([hid_dim]).astype('float32')))\n","\n","    def function(self, state_c, state_h, x):\n","        i = # WRITE ME\n","        f = # WRITE ME\n","        o = # WRITE ME\n","        c = # WRITE ME\n","        h = # WRITE ME\n","        return c, h\n","\n","    def forward(self, x, len_seq_max=0, init_state_c=None, init_state_h=None):\n","        x = x.transpose(0, 1)  # 系列のバッチ処理のため、次元の順番を「系列、バッチ」の順に入れ替える\n","        state_c = init_state_c\n","        state_h = init_state_h\n","        if init_state_c is None:  # 初期値を設定しない場合は0で初期化する\n","            state_c = torch.zeros((x[0].size()[0], self.hid_dim)).to(x.device)\n","        if init_state_h is None:  # 初期値を設定しない場合は0で初期化する\n","            state_h = torch.zeros((x[0].size()[0], self.hid_dim)).to(x.device)\n","\n","        size = list(state_h.unsqueeze(0).size())\n","        size[0] = 0\n","        output = torch.empty(size, dtype=torch.float).to(x.device)  # 一旦空テンソルを定義して順次出力を追加する\n","\n","        if len_seq_max == 0:\n","            len_seq_max = x.size(0)\n","        for i in range(len_seq_max):\n","            state_c, state_h = self.function(state_c, state_h, x[i])\n","            output = torch.cat([output, state_h.unsqueeze(0)])  # 出力系列の追加\n","        return output"]},{"cell_type":"markdown","metadata":{"id":"IfHaLvJJWHeI"},"source":["### 2. 分類器"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mU-z8OtZu_cO"},"outputs":[],"source":["class SequenceTaggingNet3(nn.Module):\n","    def __init__(self, word_num, emb_dim, hid_dim):\n","        super().__init__()\n","        self.emb = Embedding(emb_dim, word_num)\n","        self.lstm = LSTM(emb_dim, hid_dim)\n","        self.linear = nn.Linear(hid_dim, 1)\n","\n","    def forward(self, x, len_seq_max=0, len_seq=None, init_state=None):\n","        h = # WRITE ME\n","        h = # WRITE ME\n","\n","        if len_seq is not None:\n","            # 系列が終わった時点での出力を取る必要があるので len_seq を元に集約する\n","            h = h[len_seq - 1, list(range(len(x))), :]\n","        else:\n","            h = h[-1]\n","\n","        y = # WRITE ME\n","\n","        return y"]},{"cell_type":"markdown","metadata":{"id":"qR2iKUy7yA3R"},"source":["### 3. 学習\n","\n","同じタスクを用いてRNNとLSTMの性能を比較します．"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":359576,"status":"ok","timestamp":1682757417257,"user":{"displayName":"Sensho Nobe","userId":"10041880977514616020"},"user_tz":-540},"id":"OAyD__D8vLvi","outputId":"75e539ac-bf00-4241-a4f2-346dd7583bb1"},"outputs":[{"name":"stdout","output_type":"stream","text":["EPOCH: 0, Train Loss: 0.681, Valid Loss: 0.605, Validation F1: 0.696\n","EPOCH: 1, Train Loss: 0.676, Valid Loss: 0.650, Validation F1: 0.674\n","EPOCH: 2, Train Loss: 0.608, Valid Loss: 0.533, Validation F1: 0.750\n","EPOCH: 3, Train Loss: 0.442, Valid Loss: 0.355, Validation F1: 0.856\n","EPOCH: 4, Train Loss: 0.288, Valid Loss: 0.317, Validation F1: 0.867\n"]}],"source":["emb_dim = 100\n","hid_dim = 50\n","n_epochs = 5\n","device = 'cuda'\n","\n","net = SequenceTaggingNet3(word_num, emb_dim, hid_dim)\n","net.to(device)\n","optimizer = optim.Adam(net.parameters())\n","\n","train(net, optimizer, n_epochs)"]},{"cell_type":"markdown","metadata":{"id":"thf8W0lywagD"},"source":["### 4. `nn.LSTM`を用いたネットワークの記述\n","\n","LSTMもRNNと同様に`nn.LSTM`を用いて実装することができます．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dXqI364xviGx"},"outputs":[],"source":["class SequenceTaggingNet4(nn.Module):\n","    def __init__(self, word_num, emb_dim, hid_dim):\n","        super().__init__()\n","        self.emb = nn.Embedding(word_num, emb_dim)\n","        self.lstm = nn.LSTM(emb_dim, hid_dim, 1, batch_first=True)  # nn.LSTMの使用\n","        self.linear = nn.Linear(hid_dim, 1)\n","\n","    def forward(self, x, len_seq_max=0, len_seq=None, init_state=None):\n","        h = self.emb(x)\n","        if len_seq_max > 0:\n","            h, _ = self.lstm(h[:, 0:len_seq_max, :], init_state)\n","        else:\n","            h, _ = self.lstm(h, init_state)\n","        h = h.transpose(0, 1)\n","        if len_seq is not None:\n","            h = h[len_seq - 1, list(range(len(x))), :]\n","        else:\n","            h = h[-1]\n","        y = self.linear(h)\n","\n","        return y"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":105534,"status":"ok","timestamp":1682757522785,"user":{"displayName":"Sensho Nobe","userId":"10041880977514616020"},"user_tz":-540},"id":"s6D8KzZcw0pv","outputId":"4a77dce4-ae15-42b3-c0a7-e47e598cd744"},"outputs":[{"name":"stdout","output_type":"stream","text":["EPOCH: 0, Train Loss: 0.671, Valid Loss: 0.645, Validation F1: 0.635\n","EPOCH: 1, Train Loss: 0.602, Valid Loss: 0.569, Validation F1: 0.713\n","EPOCH: 2, Train Loss: 0.508, Valid Loss: 0.548, Validation F1: 0.770\n","EPOCH: 3, Train Loss: 0.451, Valid Loss: 0.466, Validation F1: 0.792\n","EPOCH: 4, Train Loss: 0.587, Valid Loss: 0.671, Validation F1: 0.719\n","EPOCH: 5, Train Loss: 0.496, Valid Loss: 0.513, Validation F1: 0.759\n","EPOCH: 6, Train Loss: 0.441, Valid Loss: 0.526, Validation F1: 0.760\n","EPOCH: 7, Train Loss: 0.466, Valid Loss: 0.676, Validation F1: 0.363\n","EPOCH: 8, Train Loss: 0.604, Valid Loss: 0.583, Validation F1: 0.689\n","EPOCH: 9, Train Loss: 0.475, Valid Loss: 0.468, Validation F1: 0.785\n"]}],"source":["emb_dim = 100\n","hid_dim = 50\n","n_epochs = 10\n","device = 'cuda'\n","\n","net = SequenceTaggingNet4(word_num, emb_dim, hid_dim)\n","net.to(device)\n","optimizer = optim.Adam(net.parameters())\n","\n","train(net, optimizer, n_epochs)"]},{"cell_type":"markdown","metadata":{"id":"Zj0XA0PQP0gG"},"source":["## 課題3. 双方向LSTM"]},{"cell_type":"markdown","metadata":{"id":"L9ZNn5gTP2yH"},"source":["### 1. BidirectionalLSTM\n","\n","双方向LSTMでは，順方向にシークエンスを処理するLSTM（`self.forward_lstm`）と逆方向にシークエンスを処理するLSTM（`self.backward_lstm`）を別々に用意し，それぞれの出力を結合したものを全結合層へ渡します．  \n","これによりパラメータ数は増えますが，未来方向からの文脈も考慮した予測を行うことができるようになります．\n","\n","[nn.LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)において`bidirectional=True`とするだけで実現できますが，ここではその処理を理解するために実装を行います．\n","\n","- ここではこれまで`SequenceTaggingNet`として実装してきた分類器も含めて`BidirectionalLSTM`クラスとしています．（実装上どちらでも構わない）"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XefPHJEyP5vY"},"outputs":[],"source":["class BidirectionalLSTM(nn.Module):\n","    def __init__(self, word_num, emb_dim, hid_dim):\n","        super().__init__()\n","        # 順方向と逆方向のLSTMを用意する\n","        self.emb = nn.Embedding(word_num, emb_dim)\n","        self.forward_lstm = nn.LSTM(emb_dim, hid_dim, 1, batch_first=True)\n","        self.backward_lstm = nn.LSTM(emb_dim, hid_dim, 1, batch_first=True)\n","        self.linear = nn.Linear(hid_dim*2, 1) # ForwardとBackwardの出力をconcatしたものを渡すので2倍\n","\n","    def forward(self, x, len_seq_max=0, len_seq=None, init_state=None):\n","        h = self.emb(x) # (batch_size, seq_length, emb_dim)\n","\n","        # Backwardにはシークエンスを反転して渡す\n","        if len_seq_max > 0:\n","            h1, _ = self.forward_lstm(h[:, 0:len_seq_max, :], init_state)\n","            h2, _ = self.backward_lstm(torch.flip(h[:, 0:len_seq_max, :], dims=[1]), init_state)\n","        else:\n","            h1, _ = self.forward_lstm(h, init_state) # (batch_size, seq_length, hid_dim)\n","            h2, _ = self.backward_lstm(torch.flip(h, dims=[1]), init_state) # (batch_size, seq_length, hid_dim)\n","        # Backwardから返ってきたものを再び反転する\n","        h2 = torch.flip(h2, dims=[1])\n","\n","        # ForwardとBackwardの出力を結合\n","        h = torch.cat([h1, h2], dim=2).transpose(0, 1)\n","\n","        if len_seq is not None:\n","            h = h[len_seq - 1, list(range(len(x))), :]\n","        else:\n","            h = h[-1]\n","\n","        y = self.linear(h)\n","\n","        return y"]},{"cell_type":"markdown","metadata":{"id":"1xbAI25fQA2E"},"source":["### 2. 学習\n","\n","Bidirectional LSTMでも同じタスクを行います．"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":118680,"status":"ok","timestamp":1682757641462,"user":{"displayName":"Sensho Nobe","userId":"10041880977514616020"},"user_tz":-540},"id":"3UJx334FQBbK","outputId":"8bf4d717-f5ab-4b1e-b203-808a06cd1621"},"outputs":[{"name":"stdout","output_type":"stream","text":["EPOCH: 0, Train Loss: 0.670, Valid Loss: 0.611, Validation F1: 0.645\n","EPOCH: 1, Train Loss: 0.565, Valid Loss: 0.531, Validation F1: 0.740\n","EPOCH: 2, Train Loss: 0.523, Valid Loss: 0.511, Validation F1: 0.747\n","EPOCH: 3, Train Loss: 0.407, Valid Loss: 0.442, Validation F1: 0.794\n","EPOCH: 4, Train Loss: 0.364, Valid Loss: 0.415, Validation F1: 0.815\n","EPOCH: 5, Train Loss: 0.328, Valid Loss: 0.530, Validation F1: 0.717\n","EPOCH: 6, Train Loss: 0.281, Valid Loss: 0.394, Validation F1: 0.842\n","EPOCH: 7, Train Loss: 0.252, Valid Loss: 0.426, Validation F1: 0.840\n","EPOCH: 8, Train Loss: 0.217, Valid Loss: 0.436, Validation F1: 0.832\n","EPOCH: 9, Train Loss: 0.235, Valid Loss: 0.428, Validation F1: 0.847\n"]}],"source":["emb_dim = 100\n","hid_dim = 50\n","n_epochs = 10\n","device = 'cuda'\n","\n","net = BidirectionalLSTM(word_num, emb_dim, hid_dim)\n","net.to(device)\n","optimizer = optim.Adam(net.parameters())\n","\n","train(net, optimizer, n_epochs)"]},{"cell_type":"markdown","metadata":{"id":"ExuiSiTo2k3m"},"source":["## 【補足】Gradient Clippingによる長系列への対処\n","\n","LSTMは長系列に対しても学習がうまく行きやすいモデルでしたが，一般のRNNにおける長系列の学習の工夫として，**Gradient Clipping**に触れておきます．\n","\n","RNNでは誤差逆伝播法が特に**Back Propagation Through Time (BPTT)**と呼ばれるものになり，各層のみならず各時点の勾配が乗算されます．\n","\n","そのため，通常よりも勾配が過大（或いは過小）になりやすいという特徴をもっています．\n","\n","こうした現象を**勾配爆発（消失）**と呼びますが，勾配爆発は学習を不安定化し収束を困難にします．\n","\n","![Clipping](../figures/Clipping.png)\n","出典：Ian Goodfellow et. al, “Deep Learning”, MIT press, 2016 (http://www.deeplearningbook.org/)\n","\n","そこで，勾配の大きさを意図的に制限して対処しようというのが，Gradient Clippingと呼ばれる手法です．\n","\n","以下のように，`torch.nn.utils.clip_grad_norm_(parameters, max_norm)`を用いることで勾配をclippingすることができます．\n","\n","この関数はparametersにmodelのパラメータ，max_normに勾配の絶対値の最大値を取ることで用いることができます．"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lpu3TlsbxAuK"},"outputs":[],"source":["def train_gradient_clipping(\n","    net,\n","    optimizer,\n","    n_epochs,\n","):\n","    for epoch in range(n_epochs):\n","        losses_train = []\n","        losses_valid = []\n","\n","        net.train()\n","        n_train = 0\n","        acc_train = 0\n","        for label, line, len_seq in train_dataloader:\n","\n","            net.zero_grad()\n","\n","            t = label.to(device)  # テンソルをGPUに移動\n","            x = line.to(device) # ( batch, time )\n","            len_seq.to(device)\n","\n","            h = net(x, torch.max(len_seq), len_seq)\n","            y = torch.sigmoid(h).squeeze()\n","\n","            loss = -torch.mean(t*torch_log(y) + (1 - t)*torch_log(1 - y))\n","\n","            loss.backward()\n","\n","            # 勾配を絶対値1.0でクリッピングする\n","            torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n","\n","            optimizer.step()\n","\n","            losses_train.append(loss.tolist())\n","\n","            n_train += t.size()[0]\n","\n","        t_valid = []\n","        y_pred = []\n","        net.eval()\n","        for label, line, len_seq in valid_dataloader:\n","\n","            t = label.to(device)  # テンソルをGPUに移動\n","            x = line.to(device) # ( batch, time )\n","            len_seq.to(device)\n","\n","            h = net(x, torch.max(len_seq), len_seq)\n","            y = torch.sigmoid(h).squeeze()\n","\n","            loss = -torch.mean(t*torch_log(y) + (1 - t)*torch_log(1 - y))\n","\n","            pred = y.round().squeeze()\n","\n","            t_valid.extend(t.tolist())\n","            y_pred.extend(pred.tolist())\n","\n","            losses_valid.append(loss.tolist())\n","\n","        print('EPOCH: {}, Train Loss: {:.3f}, Valid Loss: {:.3f}, Validation F1: {:.3f}'.format(\n","            epoch,\n","            np.mean(losses_train),\n","            np.mean(losses_valid),\n","            f1_score(t_valid, y_pred, average='macro')\n","        ))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":96271,"status":"ok","timestamp":1682757737727,"user":{"displayName":"Sensho Nobe","userId":"10041880977514616020"},"user_tz":-540},"id":"9MbcEGef5okD","outputId":"04ca1356-6071-4c8f-af41-087434a07d6b"},"outputs":[{"name":"stdout","output_type":"stream","text":["EPOCH: 0, Train Loss: 0.695, Valid Loss: 0.693, Validation F1: 0.409\n","EPOCH: 1, Train Loss: 0.692, Valid Loss: 0.693, Validation F1: 0.440\n","EPOCH: 2, Train Loss: 0.686, Valid Loss: 0.693, Validation F1: 0.452\n","EPOCH: 3, Train Loss: 0.670, Valid Loss: 0.703, Validation F1: 0.422\n","EPOCH: 4, Train Loss: 0.650, Valid Loss: 0.705, Validation F1: 0.460\n","EPOCH: 5, Train Loss: 0.623, Valid Loss: 0.717, Validation F1: 0.485\n","EPOCH: 6, Train Loss: 0.599, Valid Loss: 0.735, Validation F1: 0.503\n","EPOCH: 7, Train Loss: 0.572, Valid Loss: 0.756, Validation F1: 0.489\n","EPOCH: 8, Train Loss: 0.550, Valid Loss: 0.779, Validation F1: 0.490\n","EPOCH: 9, Train Loss: 0.529, Valid Loss: 0.803, Validation F1: 0.495\n"]}],"source":["emb_dim = 100\n","hid_dim = 50\n","n_epochs = 10\n","device = 'cuda'\n","\n","net = SequenceTaggingNet2(word_num, emb_dim, hid_dim)\n","net.to(device)\n","optimizer = optim.Adam(net.parameters())\n","\n","train_gradient_clipping(net, optimizer, n_epochs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PWET60FvikgK"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"nam22","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"vscode":{"interpreter":{"hash":"4928584a19934792772f8bb8909943c28c334459e9e815fcc970144b80af4840"}}},"nbformat":4,"nbformat_minor":0}
