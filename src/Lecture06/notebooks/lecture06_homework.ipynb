{"cells":[{"cell_type":"markdown","metadata":{"id":"SI0CCRYGbsLX"},"source":["# 第6回講義 宿題"]},{"cell_type":"markdown","metadata":{"id":"jhZjl_2gbsLf"},"source":["## 課題\n","RNNを用いてIMDbのsentiment analysisを実装してみましょう．\n","\n","ネットワークの形などに制限はとくになく，今回のLessonで扱った内容以外の工夫も組み込んでもらって構いません．"]},{"cell_type":"markdown","metadata":{"id":"NBcfs2VybsLj"},"source":["## 目標値\n","F値：0.85"]},{"cell_type":"markdown","metadata":{"id":"tZj-U7SvbsLl"},"source":["## ルール\n","- 以下のセルで指定されている`x_train`, `t_train`以外の学習データは使わないでください．"]},{"cell_type":"markdown","metadata":{"id":"jm9ZJX2TbsLo"},"source":["## 提出方法\n","- 2つのファイルを提出していただきます．\n","  1. テストデータ `x_test` に対する予測ラベルを`submission_pred.csv`として保存し，Omnicampusの宿題から「第6回 回帰結合型ニューラルネットワーク」を選択して提出してください．\n","  2. それに対応するpythonのコードを`submission_code.py`として保存し，Omnicampusの宿題から「第6回 回帰結合型ニューラルネットワーク (code)」を選択して提出してください．\n","    - セルに書いたコードを.py形式で保存するためには%%writefileコマンドなどを利用してください．\n","    - writefileコマンドではファイルの保存のみが行われセル内のpythonコード自体は実行されません．そのため，実際にコードを走らせる際にはwritefileコマンドをコメントアウトしてください．\n","\n","\n","- コードの内容を変更した場合は，1と2の両方を提出し直してください．\n","\n","- なお採点は1で行い，2はコードの確認用として利用します．(成績優秀者はコード内容を公開させていただくかもしれません)\n"]},{"cell_type":"markdown","metadata":{"id":"ejdA6CESbsLs"},"source":["## 評価方法\n","\n","- 予測ラベルの`t_test`に対するF値で評価します．\n","- 即時採点しLeader Boardを更新します．（採点スケジュールは別アナウンス）\n","- 締切時の点数を最終的な評価とします．\n","\n"]},{"cell_type":"markdown","metadata":{"id":"IjSgBf1ibsLw"},"source":["## データの読み込み（このセルは修正しないでください）"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18275,"status":"ok","timestamp":1685612802680,"user":{"displayName":"Sensho Nobe","userId":"10041880977514616020"},"user_tz":-540},"id":"4X9ptijybsLz","outputId":"74f3b82e-e81e-482d-ef8b-919ad26a91df"},"outputs":[{"name":"stdout","output_type":"stream","text":["単語種数: 88587\n"]}],"source":["import random\n","import os\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.autograd as autograd\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from torch.nn.utils.rnn import pad_sequence\n","from torchtext import datasets\n","from torchtext.vocab import vocab\n","from torchtext.data.utils import get_tokenizer\n","from sklearn.metrics import f1_score\n","from sklearn.model_selection import train_test_split\n","from collections import Counter\n","import pandas as pd\n","import string\n","import re\n","from typing import List, Union\n","import torchtext\n","\n","torchtext.disable_torchtext_deprecation_warning()\n","\n","seed = 1234\n","torch.manual_seed(seed)\n","np.random.seed(seed)\n","random.seed(seed)\n","\n","\n","# 学習データ\n","data_dir = '../data'\n","x_train = np.load(os.path.join(data_dir, 'x_train.npy'), allow_pickle=True)\n","t_train = np.load(os.path.join(data_dir, 't_train.npy'), allow_pickle=True)\n","\n","# 検証データを取る\n","x_train, x_valid, t_train, t_valid = train_test_split(x_train, t_train, test_size=0.2, random_state=seed)\n","    \n","# テストデータ\n","x_test = np.load(os.path.join(data_dir, 'x_test.npy'), allow_pickle=True)\n","\n","\n","def text_transform(text: List[int], max_length=256):\n","    # <BOS>はすでに1で入っている．<EOS>は2とする．\n","    text = text[:max_length - 1] + [2]\n","\n","    return text, len(text)\n","\n","def collate_batch(batch):\n","    label_list, text_list, len_seq_list = [], [], []\n","    \n","    for sample in batch:\n","        if isinstance(sample, tuple):\n","            label, text = sample\n","\n","            label_list.append(label)\n","        else:\n","            text = sample.copy()\n","            \n","        text, len_seq = text_transform(text)\n","        text_list.append(torch.tensor(text))\n","        len_seq_list.append(len_seq)\n","        \n","    # NOTE: 宿題用データセットでは<PAD>は3です．\n","    return torch.tensor(label_list), pad_sequence(text_list, padding_value=3).T, torch.tensor(len_seq_list)\n","\n","\n","word_num = np.concatenate(np.concatenate((x_train, x_test))).max() + 1\n","print(f\"単語種数: {word_num}\")"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/plain":["(773, 117, 580)"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["len(x_train[0]), len(x_train[1]), len(x_train[2])"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of x_train: (32000,)\n","Shape of t_train: (32000,)\n","Shape of x_valid: (8000,)\n","Shape of t_valid: (8000,)\n","Shape of x_test: (10000,)\n","(42000,)\n","(9847205,)\n"]}],"source":["print(\"Shape of x_train:\", x_train.shape)\n","print(\"Shape of t_train:\", t_train.shape)\n","print(\"Shape of x_valid:\", x_valid.shape)\n","print(\"Shape of t_valid:\", t_valid.shape)\n","print(\"Shape of x_test:\", x_test.shape)\n","\n","print(np.concatenate((x_train, x_test)).shape)\n","print(np.concatenate(np.concatenate((x_train, x_test))).shape)"]},{"cell_type":"markdown","metadata":{"id":"WIKgr01wbsL5"},"source":["## 実装"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"WPhmbQtagOvH"},"outputs":[],"source":["batch_size = 128\n","\n","train_dataloader = DataLoader(\n","    [(t, x) for t, x in zip(t_train, x_train)],\n","    batch_size=batch_size,\n","    shuffle=True,\n","    collate_fn=collate_batch,\n",")\n","valid_dataloader = DataLoader(\n","    [(t, x) for t, x in zip(t_valid, x_valid)],\n","    batch_size=batch_size,\n","    shuffle=False,\n","    collate_fn=collate_batch,\n",")\n","test_dataloader = DataLoader(\n","    x_test,\n","    batch_size=batch_size,\n","    shuffle=False,\n","    collate_fn=collate_batch,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w6j2soNLbsL7"},"outputs":[],"source":["def torch_log(x):\n","    return torch.log(torch.clamp(x, min=1e-10))\n","\n","\n","class Embedding(nn.Module):\n","    # WRITE ME\n","\n","\n","class SequenceTaggingNet(nn.Module):\n","    # WRITE ME"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9w3yjqqCl_Zb"},"outputs":[],"source":["emb_dim = 100\n","hid_dim = 50\n","n_epochs = 10\n","device = 'cuda'\n","\n","net = SequenceTaggingNet(word_num, emb_dim, hid_dim)\n","net.to(device)\n","optimizer = optim.Adam(net.parameters())\n","\n","for epoch in range(n_epochs):\n","    losses_train = []\n","    losses_valid = []\n","\n","    net.train()\n","    n_train = 0\n","    acc_train = 0\n","    for label, line, len_seq in train_dataloader:\n","        \n","        # WRITE ME\n","\n","        losses_train.append(loss.tolist())\n","\n","        n_train += t.size()[0]\n","\n","    # Valid\n","    t_valid = []\n","    y_pred = []\n","    net.eval()\n","    for label, line, len_seq in valid_dataloader:\n","\n","        # WRITE ME\n","\n","        t_valid.extend(t.tolist())\n","        y_pred.extend(pred.tolist())\n","\n","        losses_valid.append(loss.tolist())\n","\n","    print('EPOCH: {}, Train Loss: {:.3f}, Valid Loss: {:.3f}, Validation F1: {:.3f}'.format(\n","        epoch,\n","        np.mean(losses_train),\n","        np.mean(losses_valid),\n","        f1_score(t_valid, y_pred, average='macro')\n","    ))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dQ8xtUcHj6IR"},"outputs":[],"source":["net.eval()\n","\n","y_pred = []\n","for _, line, len_seq in test_dataloader:\n","\n","    x = line.to(device)\n","    len_seq.to(device)\n","\n","    h = net(x, torch.max(len_seq), len_seq)\n","    y = torch.sigmoid(h).squeeze()\n","\n","    pred = y.round().squeeze()  # 0.5以上の値を持つ要素を正ラベルと予測する\n","\n","    y_pred.extend(pred.tolist())\n","\n","\n","submission = pd.Series(y_pred, name='label')\n","submission.to_csv('drive/MyDrive/Colab Notebooks/DLBasics2023_colab/Lecture06/submission_pred.csv', header=True, index_label='id')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W-MRCNboHZNd"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":0}
